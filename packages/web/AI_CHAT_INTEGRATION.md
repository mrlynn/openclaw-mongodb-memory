# AI-Powered Memory Chat Integration

The Memory Chat feature integrates with OpenClaw Gateway to provide conversational, AI-powered answers based on your stored memories.

## How It Works

### Request Flow

```
User Question
    ↓
Frontend (/chat page)
    ↓
Backend API (/api/chat)
    ↓
    ├─→ Memory Daemon (semantic search)
    │       ↓
    │   Top 5 memories retrieved
    │       ↓
    └─→ OpenClaw Gateway (LLM)
            ↓
        AI generates conversational answer
            ↓
        Response with answer + source memories
            ↓
Frontend displays conversational answer
```

### Architecture

**1. Frontend Component** (`app/chat/page.tsx`)

- Chat UI with message history
- Sends user questions to `/api/chat`
- Displays AI-generated answers
- Shows source memories as cards

**2. Backend API Route** (`app/api/chat/route.ts`)

- Searches memories via daemon `/recall` endpoint
- Builds context from top memories
- Calls OpenClaw Gateway RPC with context
- Returns conversational answer + sources

**3. Graceful Degradation**

- If OpenClaw Gateway unavailable → falls back to formatted memory list
- If no memories found → helpful "not found" message
- Always includes source memories for transparency

---

## Configuration

### Environment Variables

**Required:**

```bash
# Backend (server-side)
DAEMON_URL=http://localhost:7751           # Memory daemon
OPENCLAW_GATEWAY_URL=http://localhost:7777 # OpenClaw Gateway

# Frontend (client-side)
NEXT_PUBLIC_DAEMON_URL=http://localhost:7751
```

**File:** `packages/web/.env.local`

---

## OpenClaw Gateway Setup

### Option 1: Local Gateway (Recommended for Development)

```bash
# Ensure OpenClaw Gateway is running
openclaw gateway start

# Verify it's listening
curl http://localhost:7777/health
```

### Option 2: Remote Gateway

```bash
# Point to remote OpenClaw instance
OPENCLAW_GATEWAY_URL=https://your-openclaw-gateway.com
```

---

## API Contract

### Request to `/api/chat`

```typescript
POST /api/chat
Content-Type: application/json

{
  "query": "What did we decide about Docker setup?",
  "agentId": "openclaw"
}
```

### Response

```typescript
{
  "answer": "Based on your memories, you decided to use Docker Compose for local development...",
  "memories": [
    {
      "text": "Created docker-compose.yml...",
      "score": 0.85,
      "tags": ["docker", "setup"],
      "createdAt": "2026-02-23T12:00:00Z"
    }
  ],
  "source": "llm" | "fallback" | "error"
}
```

**Sources:**

- `llm` - Answer generated by OpenClaw Gateway LLM
- `fallback` - Simple memory list (Gateway unavailable)
- `error` - Error occurred

---

## LLM Prompt Template

The system uses this prompt structure:

```
System: You are a helpful memory assistant. Answer the user's question
based ONLY on the provided memories. Be conversational and natural.

Memories:
[Memory 1] Docker Compose created for full stack...
[Memory 2] MongoDB Atlas guide documented...
[Memory 3] Web dashboard theme updated to MongoDB brand colors...

User: What did we accomplish today?
```

The LLM:

- Reads the memories as context
- Synthesizes a conversational answer
- Stays grounded in the provided memories
- Admits when it doesn't know something

---

## Model Selection

**Default:** Haiku (fast + cheap)

```typescript
model: "haiku"; // Fast responses for chat
```

**Alternative models:**

- `sonnet` - Better quality, slower, more expensive
- `opus` - Best quality, slowest, most expensive

**To change model:**
Edit `packages/web/app/api/chat/route.ts`:

```typescript
params: {
  model: "sonnet"; // Change here
}
```

---

## Customization

### Adjust Memory Count

Change how many memories are searched:

```typescript
// In app/api/chat/route.ts
recallUrl.searchParams.set("limit", "10"); // Default: 5
```

### Customize System Prompt

Edit the prompt in `app/api/chat/route.ts`:

```typescript
const systemPrompt = `You are a friendly memory assistant...

Include:
- Key decisions
- Important dates
- Relevant context

Format your answers as:
1. Direct answer
2. Supporting details
3. Source references

Memories:
${memoryContext}`;
```

### Timeout Configuration

```typescript
// Increase timeout for slow responses
signal: AbortSignal.timeout(60000); // 60 seconds
```

---

## Fallback Behavior

When OpenClaw Gateway is **unavailable**, the system falls back to a simple formatted list:

```
I found 3 relevant memories about "Docker setup":

1. Created docker-compose.yml for full stack deployment
   (Tags: docker, setup, infrastructure)

2. Built multi-stage Dockerfiles for daemon and web
   (Tags: docker, build, optimization)

3. Tested docker compose up successfully
   (Tags: docker, testing)
```

This ensures the chat remains useful even without the LLM.

---

## Monitoring & Debugging

### Check Gateway Connection

```bash
# From web container/server
curl $OPENCLAW_GATEWAY_URL/health
```

### View API Logs

```bash
# Development mode
# Logs appear in terminal running `pnpm dev`

# Production mode
pm2 logs openclaw-memory-web
```

### Test API Directly

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What did we build today?",
    "agentId": "openclaw"
  }'
```

---

## Performance Considerations

**Typical Response Time:**

- Memory search: 50-200ms
- LLM generation (Haiku): 500-2000ms
- **Total:** ~1-2 seconds

**Token Usage (per request):**

- Input: ~500 tokens (memories + prompt + question)
- Output: ~150 tokens (answer)
- **Cost (Haiku):** ~$0.0002 per question

**Rate Limits:**

- Respects OpenClaw Gateway limits
- 30-second timeout per request
- No client-side rate limiting (relies on Gateway)

---

## Security

**API Route Protection:**

- Server-side only (Next.js API route)
- No CORS issues (same origin)
- Environment variables not exposed to client

**Memory Access:**

- Filtered by `agentId`
- No cross-agent memory leakage
- User controls which agent to query

**LLM Safety:**

- Sandboxed in OpenClaw Gateway
- No arbitrary code execution
- Grounded in provided memories only

---

## Future Enhancements

**Planned:**

1. **Conversation History** - Multi-turn conversations with context
2. **Citation Links** - Click memories to view full detail
3. **Follow-up Suggestions** - AI-suggested next questions
4. **Voice Input** - Ask questions via speech
5. **Export Chat** - Download conversation as markdown

**Advanced:** 6. **Multi-Agent Queries** - "Compare what alice and bob remember about X" 7. **Time-Based Context** - "What did I know about this last week?" 8. **Proactive Insights** - "Based on your memories, you might want to..."

---

## Troubleshooting

### "OpenClaw Gateway unavailable"

**Cause:** Gateway not running or wrong URL

**Fix:**

```bash
# Check Gateway status
openclaw gateway status

# Start if needed
openclaw gateway start

# Verify URL in .env.local
echo $OPENCLAW_GATEWAY_URL
```

### "Memory daemon unavailable"

**Cause:** Daemon not running or wrong URL

**Fix:**

```bash
# Check daemon
curl http://localhost:7751/health

# Start daemon
cd packages/daemon
pnpm run dev
```

### "Responses are slow"

**Cause:** Using Sonnet/Opus instead of Haiku

**Fix:**
Change model to `haiku` in `app/api/chat/route.ts`

### "Answers don't match my memories"

**Cause:** Semantic search not finding relevant memories

**Fix:**

- Try rephrasing question
- Add more specific keywords
- Check if memories exist: http://localhost:3000/browser

---

## Support

**Documentation:**

- OpenClaw Gateway: https://docs.openclaw.ai/gateway
- Next.js API Routes: https://nextjs.org/docs/app/api-reference

**Community:**

- Discord: https://discord.com/invite/clawd
- GitHub Issues: https://github.com/mrlynn/openclaw-mongodb-memory/issues
